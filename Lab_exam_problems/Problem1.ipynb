{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states, actions, and rewards\n",
    "states = range(1, 11)\n",
    "actions = ['continue', 'quit']\n",
    "rewards = {1: 100, 2: 500, 3: 1000, 4: 5000, 5: 10000, 6: 50000, 7: 100000, 8: 500000, 9: 1000000, 10: 5000000}\n",
    "probabilities = {1: 0.99, 2: 0.9, 3: 0.8, 4: 0.7, 5: 0.6, 6: 0.5, 7: 0.4, 8: 0.3, 9: 0.2, 10: 0.1}\n",
    "\n",
    "# Initialize the state-action value function Q(s, a)\n",
    "Q = {}\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        Q[(s, a)] = 0\n",
    "\n",
    "# Define the policy function π(a|s) as an ε-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random action with probability ε\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        # Greedy action with probability (1-ε)\n",
    "        return max(actions, key=lambda a: Q[(state, a)])\n",
    "\n",
    "# Generate episodes and update Q using first-visit MC estimation\n",
    "num_episodes = 10000\n",
    "gamma = 0.9\n",
    "returns = {(s, a): [] for s in states for a in actions}\n",
    "for i in range(num_episodes):\n",
    "    episode = []\n",
    "    state = 1\n",
    "    while True:\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        reward = rewards[state] if action == 'quit' else 0\n",
    "        if action == 'continue':\n",
    "            if np.random.uniform() < probabilities[state]:\n",
    "                state += 1\n",
    "            else:\n",
    "                action = 'quit'\n",
    "        episode.append((state, action, reward))\n",
    "        if action == 'quit':\n",
    "            break\n",
    "    G = 0\n",
    "    for t in range(len(episode)-1, -1, -1):\n",
    "        state, action, reward = episode[t]\n",
    "        G = gamma*G + reward\n",
    "        if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "            returns[(state, action)].append(G)\n",
    "            Q[(state, action)] = np.mean(returns[(state, action)])\n",
    "\n",
    "# Derive the optimal policy and compute the value function\n",
    "optimal_policy = {}\n",
    "value_function = {}\n",
    "for s in states:\n",
    "    optimal_policy[s] = max(actions, key=lambda a: Q[(s, a)])\n",
    "    value_function[s] = max(Q[(s, a)] for a in actions)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "print(\"Value Function:\")\n",
    "print(value_function)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
